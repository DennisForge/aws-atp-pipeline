# AWS Lambda ATP Pipeline Project  
![AWS Lambda](https://img.shields.io/badge/AWS-Lambda-orange?logo=awslambda)  
![Amazon S3](https://img.shields.io/badge/Amazon-S3-green?logo=amazons3)  
![EventBridge](https://img.shields.io/badge/Amazon-EventBridge-red?logo=amazonaws)  
![Python 3.13](https://img.shields.io/badge/Python-3.13-blue?logo=python)  
![Serverless](https://img.shields.io/badge/Architecture-Serverless-lightgrey)

This project demonstrates a complete serverless data automation workflow built with AWS Lambda, Amazon S3, EventBridge, and the Kaggle API. The Lambda function automatically downloads ATP tennis match data from Kaggle, uploads it to an S3 bucket, processes the data, and generates a summary file listing the Top 50 tennis players by total wins. The entire setup was created and tested through the AWS Management Console.

## Architecture
```
EventBridge → Lambda → Kaggle API → S3 (Raw) → Transform → S3 (Results)
                ↑           ↓
         Secrets Manager   Pre-signed URL
```

## Step-by-Step Implementation
1. **S3 Bucket Setup**  
A new S3 bucket was created to store both raw and processed data files. Inside the bucket, two directories (prefixes) were manually created:  
- `raw/` → stores the original CSV files downloaded from Kaggle  
- `results/` → stores processed CSV reports generated by the Lambda function  
This structure follows proper ETL design, ensuring clear separation between source data and output data.

2. **Kaggle Credentials via Secrets Manager**  
To enable secure API access to Kaggle, credentials were stored in AWS Secrets Manager under the name `kaggle/credentials`.  
The secret was saved in JSON format:  
```json
{  
  "KAGGLE_USERNAME": "your_username",  
  "KAGGLE_KEY": "your_key"  
}
```
The Lambda environment variable `SECRET_ARN` was set to the ARN of this secret so the function could load credentials dynamically at runtime without exposing them in code.

3. **Lambda Function Configuration**  
A new AWS Lambda function was created in the eu-north-1 (Stockholm) region.  
- **Runtime:** Python 3.13  
- **Memory:** 1024 MB  
- **Timeout:** 60 seconds  

Environment variables:  
```bash
S3_BUCKET=atp-analysis-demo  
RAW_PREFIX=raw/  
RESULTS_PREFIX=results/  
RAW_OBJECT=atp_tennis.csv  
SECRET_ARN=arn:aws:secretsmanager:eu-north-1:...:secret:kaggle/credentials-...  
DATASET_SLUG=dissfya/atp-tennis-2000-2023daily-pull  
```

4. **Adding Python Layers**  
Since AWS Lambda does not include all required libraries by default, two layers were added manually:  
- `boto3` → for AWS SDK operations (S3, Secrets Manager)  
- `requests` → for Kaggle API communication  
Dependencies are handled through Lambda Layers, so no local requirements file is needed.

5. **Uploading and Running the Python Script**  
The main script `lambda_function.py` was uploaded directly into the Lambda editor. The script performs:  
- Check if the CSV file already exists in the S3 `raw/` folder  
- If not, download the dataset ZIP from Kaggle using Basic Auth  
- Extract and upload the CSV to `raw/`  
- Process the data to calculate total wins per player  
- Generate a Top 50 list based on total wins  
- Save the processed file to `results/`  
- Generate a pre-signed URL valid for 7 days for downloading the result  

Example output:  
```json
{  
  "status": "ok",  
  "raw_key": "raw/atp_tennis.csv",  
  "output_key": "results/atp-top-50-04-11-2025.csv",  
  "rows": 50,  
  "presigned_url_7d": "https://your-bucket.s3.amazonaws.com/results/atp-top-50-04-11-2025.csv?..."  
}
```

6. **EventBridge Scheduler Configuration**  
To automate execution, an EventBridge rule was created with the following configuration:  
- **Rule name:** `atp-top50-weekly`  
- **Schedule expression:** `cron(0 6 ? * MON *)` → runs every Monday at 06:00 UTC  
The rule was linked to the Lambda manually using Add Trigger → Existing rule → atp-top50-weekly.

7. **Testing and Validation**  
The function was tested manually using the Lambda console. Execution logs confirmed:  
- Data successfully fetched from Kaggle  
- CSV uploaded to `raw/` in S3  
- Processed Top 50 CSV created in `results/`  
- Pre-signed URL returned successfully  
All logs were verified in CloudWatch, confirming correct execution.

## Output Format
Results CSV contains Top 50 players by total wins:

| Column | Description | Example |
|--------|-------------|---------|
| `player_name` | Tennis player name | `Federer R.` |
| `total_wins` | Total tournament wins | `1157` |
| `grand_slem_wins` | Grand Slam victories | `362` |
| `atp1000_wins` | Masters 1000 wins | `431` |
| `atp500_wins` | ATP 500 wins | `109` |
| `first_win` | Date of first recorded win | `2000-01-03` |
| `last_win` | Date of most recent win | `2021-07-05` |

### Sample Top 5 Results
```csv
Federer R.,1157,362,431,109,2000-01-03,2021-07-05
Djokovic N.,1061,384,458,99,2004-09-14,2025-10-09
Nadal R.,1009,303,424,120,2003-04-15,2024-07-20
Ferrer D.,678,139,194,107,2002-07-15,2019-05-07
Murray A.,675,195,239,86,2005-06-06,2024-06-18
```

## Problems and Fixes
- **403 Forbidden Error:** Kaggle API returned 403 because credentials were incorrectly named `USERNAME` and `KEY`. Fixed by renaming to `KAGGLE_USERNAME` and `KAGGLE_KEY` and regenerating the token.  
- **Missing Trigger Connection:** The EventBridge rule was created but not linked. Fixed by manually attaching it to the Lambda.  
- **Constant JSON Input Confusion:** AWS Console requested JSON input. Fixed by providing an empty JSON `{}`.  
- **No Logs in S3:** Lambda logs are stored in CloudWatch, not S3. Verified logs in CloudWatch.

## Security Features
- ✅ No hardcoded credentials - all sensitive data in Secrets Manager  
- ✅ Least privilege IAM permissions  
- ✅ Environment-based configuration  
- ✅ Input validation and error handling  

## Manual Testing
```bash
# Test the Lambda function directly
aws lambda invoke \
  --function-name atp-tennis-pipeline \
  --payload '{}' \
  response.json
```
Note: Replace `atp-tennis-pipeline` with your actual Lambda function name before running.

## Final Results
After fixes, the system runs fully automatically. The Lambda function performs a full ETL pipeline:  
1. **Extract** – Downloads raw data from Kaggle  
2. **Transform** – Processes and aggregates data into a summary table  
3. **Load** – Uploads processed CSV to S3 and generates a 7-day pre-signed URL  
Average runtime is under 2 seconds with 102 MB of memory used.  

The process runs weekly without manual input.

## Tools and Services Used
AWS Lambda, Amazon S3, Amazon EventBridge, AWS Secrets Manager, Amazon CloudWatch, Python 3.13, Requests, Boto3, Kaggle API.

## Summary
This project demonstrates a fully automated serverless ETL workflow in AWS. It retrieves, transforms, and delivers updated sports data on a schedule with zero infrastructure management. The design can easily be extended with another Lambda (transformer) for further data processing or analytics automation.  
This project showcases the ability to design, deploy, and debug serverless ETL workflows using core AWS services.
